{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ffff3a-708a-477d-b732-a266516a134e",
   "metadata": {},
   "source": [
    "Adam Whelpley\n",
    "Prof. Elam\n",
    "CS379 Machine Learning\n",
    "11/3/2024\n",
    "\n",
    "Individual Project 4\n",
    "\n",
    "This model uses the LSTM to make predictions based upon fed data. In short, the LSTM reads a data file (in this instance Alice in Wonderland) and uses the patterns from the story to generate a text file similar to the story. The following code generates the story by looking at the probability of the next character in a sequence. For example, if the algorithm sees that the current character is a 'q', and the most recent was a space, it will predict that the next characters might be 'u', 'e', 'e', and 'n' to make the word \"queen\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e27b1-e97f-4ad7-b49d-de655eb1511e",
   "metadata": {},
   "source": [
    "Importing proper packages. These packages are commonly used with the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc03c58-a75c-4657-b0d8-8528496f0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf49c6-470a-4a5f-9f25-d63ac5c02fe4",
   "metadata": {},
   "source": [
    "The text needs to be converted to raw data for the algorithm to do work on it. To do so, the following opens the text file and reads it into a data structure. The text is put to all lowercase for simplicity and gives the code the ability to distinguish words as the same. Otherwise, the word 'Dog' and 'dog' would be read as two different words, when they are in fact the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a898fe51-2656-4c02-b504-367d27c5857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = open(\"C:\\\\Users\\\\pwhel\\\\School\\\\Machine Learning\\\\Unit 4\\\\text\\\\alice_in_wonderland.txt\",encoding='UTF-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199b28c6-5dd3-46d4-a90f-391d89687ab8",
   "metadata": {},
   "source": [
    "For the computer to understand the data, the text characters must be converted to numbers. We can do so with the following code to give distinct characters an assigned numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae9122d-6ad6-46e1-88ca-f0327ddedaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f3d72-21f1-495c-8a41-5d1025bc9d78",
   "metadata": {},
   "source": [
    "The data is then split to dinstinguish a certain number of characters to train the model and another number of characters to test the validity of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "241c5e15-c59d-4ec4-b5bd-c59264bd8400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataTrain = []\n",
    "dataTest = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataTrain.append([char_to_int[char] for char in seq_in])\n",
    "    dataTest.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataTrain)\n",
    "n_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b62ec6-ef9a-4ea0-acdb-ec1d7f776af7",
   "metadata": {},
   "source": [
    "Characters are hot coded to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa87e86-8963-4966-95a8-776fef1e6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = numpy.reshape(dataTrain, (n_patterns, seq_length, 1))\n",
    "train = train / float(n_vocab)\n",
    "test = np_utils.to_categorical(dataTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a69a1-3161-4d10-9718-d4c9a4d3e57c",
   "metadata": {},
   "source": [
    "In the next session, we define the LSTM model using a 256 unit memory and a dropout probability of 20%. The probability of prediction is between 0 and 1, representing probability between 0 and 100 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae3f090-569b-4a1b-8313-c51988c3e940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pwhel\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(train.shape[1], train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(test.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79e7dd-2b17-4be0-b0ef-49868627c669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c8c4b21a-aff7-4c26-9d87-fbb2d557215f",
   "metadata": {},
   "source": [
    "Defining checkpoints for the code to evaluate. Increasing the epochs betters the model's interpretation of words, meaning that if this number were changed from 10 to a larger number, the story generated by the model would be more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84d8c80-4708-4fc2-a078-b84579f5c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - loss: 2.1767\n",
      "Epoch 1: loss improved from inf to 2.15646, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-01-2.1565-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 186ms/step - loss: 2.1767\n",
      "Epoch 2/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - loss: 2.0660\n",
      "Epoch 2: loss improved from 2.15646 to 2.04498, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-02-2.0450-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 187ms/step - loss: 2.0660\n",
      "Epoch 3/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - loss: 1.9624\n",
      "Epoch 3: loss improved from 2.04498 to 1.94477, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-03-1.9448-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 184ms/step - loss: 1.9623\n",
      "Epoch 4/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.8690\n",
      "Epoch 4: loss improved from 1.94477 to 1.86625, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-04-1.8663-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 181ms/step - loss: 1.8690\n",
      "Epoch 5/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step - loss: 1.8021\n",
      "Epoch 5: loss improved from 1.86625 to 1.80073, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-05-1.8007-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 179ms/step - loss: 1.8021\n",
      "Epoch 6/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - loss: 1.7423\n",
      "Epoch 6: loss improved from 1.80073 to 1.74507, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-06-1.7451-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 182ms/step - loss: 1.7423\n",
      "Epoch 7/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.7020\n",
      "Epoch 7: loss improved from 1.74507 to 1.69523, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-07-1.6952-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 181ms/step - loss: 1.7019\n",
      "Epoch 8/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.6465\n",
      "Epoch 8: loss improved from 1.69523 to 1.65290, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-08-1.6529-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 181ms/step - loss: 1.6465\n",
      "Epoch 9/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - loss: 1.6074\n",
      "Epoch 9: loss improved from 1.65290 to 1.61555, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-09-1.6155-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 189ms/step - loss: 1.6074\n",
      "Epoch 10/10\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - loss: 1.5770\n",
      "Epoch 10: loss improved from 1.61555 to 1.58024, saving model to C:\\Users\\pwhel\\School\\Machine Learning\\Unit 4\\text\\Weights-LSTM-improvement-10-1.5802-bigger.keras\n",
      "\u001b[1m2320/2320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 193ms/step - loss: 1.5770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a5997ec710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"C:\\\\Users\\\\pwhel\\\\School\\\\Machine Learning\\\\Unit 4\\\\text\\\\Weights-LSTM-improvement-{epoch:02d}-{loss:.4f}-bigger.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(train, test, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2917edf-9eec-4a39-aedc-9eedb7f860b8",
   "metadata": {},
   "source": [
    "Finishing code runs the model to generate text based on the information it was fed. In this instance, it is taking the data from Alice in Wonderland and using patterns from the characters in the story to best predict the next character that would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e23d742-4127-4e84-9884-63641cedcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: \n",
      "\" e, broken only\n",
      "by an occasional exclamation of \"\n",
      " the took of the garter to hare to she see, and the queen surned and then a little shrter to her in a low voice, `nd the queen sureed the would ce taid to her in a low voice, `i dan tee you mean the batt tide in the sea, `nd the queen was a little gallen of the looked at the court with the cook and the thoe would be the little gardeners in the court with the court, and the queen surtle to the table to the court with the court, and the queen surtle to the table to the court with the court, and the queen surtle to the table to the court with the court, and the queen surtle to the table to the court with the court, and the queen surtle to the table to the court with the court, and the queen sur\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "seq_length = 46\n",
    "dataTrain = []\n",
    "dataTest = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataTrain.append([char_to_int[char] for char in seq_in])\n",
    "    dataTest.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataTrain)\n",
    "\n",
    "train = numpy.reshape(dataTrain, (n_patterns, seq_length, 1))\n",
    "train = train / float(n_vocab)\n",
    "test = np_utils.to_categorical(dataTrain)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(train.shape[1], train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(test.shape[1], activation='softmax'))\n",
    "\n",
    "model.load_weights(\"C:\\\\Users\\\\pwhel\\\\School\\\\Machine Learning\\\\Unit 4\\\\text\\\\Weights-LSTM-improvement-20-1.4705-bigger.keras\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = numpy.random.randint(0, len(dataTrain)-1)\n",
    "pattern = dataTrain[start]\n",
    "print(\"seed: \")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "for i in range(700):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4337afd-329d-417e-8b79-84715123ab89",
   "metadata": {},
   "source": [
    "The story is incoherent because running a larger amount of epochs takes more time and processing power. Again, if the number were increased the story would make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9cb37-2f20-44e6-ad4e-8e97e56c9731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
